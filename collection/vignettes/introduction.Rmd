---
title: "Refactoring Interactive R"
author: "Lukasz A. Bartnik"
date: "02.03.2015"
output: ioslides_presentation
---

## Goals/Things to Address

> * Working with data sets, where data size >> RAM
    + Interactively
    + Simple R objects stored in files
    + Consistency between objects not enforced

> * Deferring computations
    + Distributing computations in a cluster
    + Documenting work at a given stage

> * Managing artifacts
    + Input data sets (simple objects or collections, see point 1)
    + Results (data sets & objects, e.g. models)
    + Relationships (transformations, code, implementation)


## Working with Big Data Sets: Current State

* Managing data (CRAN): `ff`, `biglm`, `bigmemory`, `biganalytics`, `bigalgebra`, ...
* Third-party tools (CRAN): `SparkR`


## Working with... Collection

> - Assumptions:
    * Simulate a `list` of objects
    * Additional, pre-computed __tags__ to filter or group objects
    * Apply user code on each object/group and write the results to another __collection__
  
> - Features:
    * Interactive (help the user see the data, its contents and __tags__)
    * Apply code in parallel, possibly on a remote cluster
    * By default read-only (to preserve data consistency)
  

## Design draft

* Repository/Archive (Archivist-like?)
    + analytical/research artifacts
    + any object
    + automatic tagging
    + verbose output and object description

* Collection
    + objects storage
    + many similar objects
    + only *basic* and *user-provided* tags
    + simplified output


## Deferring Work: `defer`

* Package user-provided function
    + Add global/local dependencies if asked
    + Analyze the code and extract library function calls

* Rebuild the execution environment
    + the *same* or only *similar*?
    + make sure conflicting library calls are handled (e.g. `plyr` vs. `dplyr`)


## Distributing

* Handling data:
    + export into distributed file system (for __Spark__)
    + if based on `collection`, copy object by object

* Execution based on:
    + `Rserve` (object by object)
    + `opencpu` (object by object)
    + `SparkR` (obj-by-obj and data pre-existing in HDFS)


## Documenting Analysis/Rereach

* Serialize a **deferred task**
    + reference this object from `collection` or `archive`
    + print/plot/show using `manipulate` or `Shiny`
    + show code differences with `diff`


